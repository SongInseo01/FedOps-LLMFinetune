# Common
random_seed: 42

learning_rate: 0.0001 # Adjusted for LLM fine-tuning

model_type: 'Huggingface'  # Change to indicate Hugging Face model
model:
  _target_: transformers.AutoModelForCausalLM  # Use Hugging Face Model
  name: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'  # Replace with desired LLM model

dataset:
  name: 'medalpaca/medical_meadow_medical_flashcards'  # Input your dataset name
  llm_task: 'medical'  # Specify task type
  validation_split: 0.2  # Ratio of dividing train data by validation

# client
task_id: 'medicalllm'  # Input your Task Name that you register in FedOps Website

wandb: 
  use: false  # Whether to use wandb
  key: 'your wandb api key'  # Input your wandb api key
  account: 'your wandb account'  # Input your wandb account
  project: '${dataset.name}_${task_id}'


# server
num_epochs: 3  # Number of local epochs (adjust for better fine-tuning)
batch_size: 16  # Batch size optimized for LLM fine-tuning
num_rounds: 2  # Number of federated learning rounds
clients_per_round: 2  # Number of clients participating in each round

server:
  strategy:
    _target_: flwr.server.strategy.FedAvg  # Aggregation algorithm (default: FedAvg)
    fraction_fit: 0.1  # Adjusted to allow multiple clients if needed
    fraction_evaluate: 0.1  # Adjusted for evaluation frequency
    min_fit_clients: ${clients_per_round}  # Minimum clients for training
    min_available_clients: ${clients_per_round}  # Minimum available clients per round
    min_evaluate_clients: ${clients_per_round}  # Minimum clients for evaluation
